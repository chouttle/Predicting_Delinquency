{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CodeJam 2017 - Datadive - McGill\n",
    "---\n",
    "\n",
    "# Predicting whether a loan is going to default given data from the time of the request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal\n",
    "Our goal with this project was to train a neural network to recognize, given a set of inputs, whether a loan was going to default or not. \n",
    "We had no knowledge as to the possibility of doing such a thing, so we knew that we might have to recognize that we did not have sufficient data, sufficiently complete data, or that there might not be a clear enough pattern for us to predict the delinquency of a loan with a better than random accuracy.\n",
    "\n",
    "## Datasets\n",
    "We used the given dataset Lending_Club_Loans.csv, that contains data related to loan offered to clients of the Lending Club between 2009 and 2016 (to be verified). \n",
    "Among all the available information, we used 5 columns:\n",
    "- 4 input columns\n",
    "    - 'annual_inc': Self-reported annual income -- reported at the time at which the loan was requested\n",
    "    - 'dti': debt-to-income ratio -- reported at the time at which the loan was requested\n",
    "    - 'installment': amount to be paid monthly -- amount agreed upon between the loan requester and their debtor\n",
    "    - 'home_ownership': 'OWN', 'MORTGAGE', 'RENT', 'OTHER', 'NONE' and 'ANY' -- living situation at the time of the request\n",
    "- 1 output column\n",
    "    - 'acc_now_delinq': number of accounts delinquent at the time when this dataset was saved or recorded\n",
    "\n",
    "If we had more time, we might have used more columns, such as:\n",
    "- the employment length in years: knowing how long the creditor has held a stable job could help the accuracy\n",
    "- the month the creditor's credit line was first opened: associated with the debt-to-income ratio, we could have found some correlations\n",
    "- some columns containing textual information, such as the job title, the 'category' of the reason for the loan, or the reason given -- that would have required natural language processing with sentiment analysis, with which we have limited experience and knowledge\n",
    "\n",
    "Unfortunately, the credit score of the creditor wasn't available, otherwise, it would of course have been very valuable data. \n",
    "\n",
    "We did not use any other datasets.\n",
    "\n",
    "## Methodology\n",
    "### Data treatment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the required libraries for the project. \n",
    "\n",
    "We use the [pytorch](http://pytorch.org/) framework as well as the [pandas](http://pandas.pydata.org/) and [numpy](http://www.numpy.org/) libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data from the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loan_data = pd.read_csv(\"/home/vivien/Lending-Club-Loans.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Treatment\n",
    "We started by selectionning the columns that we wanted to use for the input and output data\n",
    "- 'annual_inc': Self-reported annual income\n",
    "- 'dti': debt-to-income ratio\n",
    "- 'installment': amount to be paid monthly\n",
    "- 'home_ownership': 'OWN', 'MORTGAGE', 'RENT', 'OTHER', 'NONE' and 'ANY'\n",
    "    - treated with 'one hot labels', with NONE and ANY considered as having no data  \n",
    "\n",
    "Then, we transformed the 'home_ownership' text data into number categories, representing the one hot labels, and concatenated the two sets to form our input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#number of parameters settings the number of neurons \n",
    "#in each layer of the neural network\n",
    "col_num = 7\n",
    "param_num = col_num\n",
    "\n",
    "#getting the relevant input columns from the data\n",
    "input_data = loan_data.ix[:,['annual_inc',\n",
    "                             'dti',\n",
    "                             'installment',\n",
    "                             'home_ownership'\n",
    "                            ]].as_matrix()\n",
    "\n",
    "#getting the relevant output columns from the data\n",
    "output_data = loan_data.ix[:,['acc_now_delinq']].as_matrix().astype('int')\n",
    "\n",
    "#temporary array to transform the text data \n",
    "#from 'home_ownership' to one hot labels\n",
    "input_home = []\n",
    "for i in range(input_data.shape[0]):\n",
    "    if input_data[i, 3] == \"OWN\":\n",
    "        input_home.append([1,0,0,0])  #OWN, #MORTGAGE, #RENT, #OTHER\n",
    "    elif input_data[i, 3] == \"MORTGAGE\":\n",
    "        input_home.append([0,1,0,0])  #OWN, #MORTGAGE, #RENT, #OTHER\n",
    "    elif input_data[i, 3] == \"RENT\":\n",
    "        input_home.append([0,0,1,0])  #OWN, #MORTGAGE, #RENT, #OTHER\n",
    "    elif input_data[i, 3] == \"OTHER\":\n",
    "        input_home.append([0,0,0,1])  #OWN, #MORTGAGE, #RENT, #OTHER\n",
    "    else:\n",
    "        input_home.append([0,0,0,0])  #OWN, #MORTGAGE, #RENT, #OTHER\n",
    "\n",
    "#removing the original 'home_ownership' column from the input_data\n",
    "input_data = np.delete(input_data, 3, 1)\n",
    "#adding the home data to the input_data\n",
    "input_data = np.concatenate((input_data, input_home), axis=1)\n",
    "input_data = input_data.astype('float')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the data\n",
    "Some of the data needed for the output is empty, so we discard those rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(887379,)\n",
      "(887350, 7)\n",
      "(887350, 1)\n"
     ]
    }
   ],
   "source": [
    "mask = output_data >= 0\n",
    "mask = np.reshape(mask, (-1))\n",
    "print(mask.shape)\n",
    "\n",
    "output_data = output_data[mask]\n",
    "input_data = input_data[mask,:]\n",
    "\n",
    "print(input_data.shape)\n",
    "print(output_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data for the output indicates the *number* of accounts that defaulted, we just want to know whether one has defaulted, so any values superior to 1 is reduced to 1, so as to have a binary category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(output_data.shape[0]):\n",
    "    if output_data[i,0] > 0:\n",
    "        output_data[i,0] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the visualization of the shapes of the input and output data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input_data:\n",
      "(887350, 7)\n",
      "Shape of output_data:\n",
      "(887350, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of input_data:\")\n",
    "print(input_data.shape)\n",
    "print(\"Shape of output_data:\")\n",
    "print(output_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separation of the defaulting data for testing (and for training with different proportion of each data)\n",
    "In order to visualize the proportion of data for each result, we made two data arrays with the \"will default\" and \"will not default\" data.  \n",
    ">For reference, 'ndef' corresponds to \"will not default\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mask_default = output_data == 1\n",
    "mask_ndef = output_data == 0\n",
    "output_data_default = output_data[mask_default]\n",
    "output_data_ndef = output_data[mask_ndef]\n",
    "\n",
    "mask_default = np.reshape(mask_default, -1)\n",
    "mask_ndef = np.reshape(mask_ndef, -1)\n",
    "input_data_default = input_data[mask_default,:]\n",
    "input_data_ndef = input_data[mask_ndef,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of loans:\n",
      "887350\n",
      "Number of defaulting loans:\n",
      "4114\n",
      "Number of loans NOT defaulting:\n",
      "883236\n",
      "Ratio of default to all\n",
      "0.004636\n",
      "In percent:\n",
      "0.464%\n"
     ]
    }
   ],
   "source": [
    "r_dta = output_data_default.shape[0]/output_data.shape[0]\n",
    "print(\"Total number of loans:\")\n",
    "print(input_data.shape[0])\n",
    "print(\"Number of defaulting loans:\")\n",
    "print(input_data_default.shape[0])\n",
    "print(\"Number of loans NOT defaulting:\")\n",
    "print(input_data_ndef.shape[0])\n",
    "print(\"Ratio of default to all\")\n",
    "print('%.6f' % r_dta)\n",
    "print(\"In percent:\")\n",
    "print('%.3f' % (r_dta*100) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there is a very small number of loans that default compared to the entire dataset.  \n",
    "**During our first attempts to train the net, we used the entire dataset, but we quickly realized that we would get better results by using a dataset with more equivalent numbers between the default and ndef.**\n",
    "This is the reason why we decided to use only as much non-defaulting loans as we had defaulting ones.  \n",
    "There are probably consequences to this decisions, but we haven't yet taken the time to examine them closely.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the data between training and testing sets\n",
    "As discussed above, we do not use the entire dataset in order to have an equal amount of defaulting and non-defaulting loans.\n",
    "\n",
    "We separate the data because it is not possible to efficiently test a neural net with the data that it was trained with. The data that is not used to train the neural net is therefore used to test it afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create the test datasets here\n",
    "#(take in account a proportion of defaulting is necessary)\n",
    "num_def = input_data_default.shape[0]\n",
    "half_num_def = int(num_def/2)\n",
    "input_data_train = np.concatenate((input_data_default[0:half_num_def], input_data_ndef[0:half_num_def]), axis=0)\n",
    "input_data_train_default = input_data_default\n",
    "input_data_train_ndef = input_data_ndef[0:num_def]\n",
    "output_data_train = np.concatenate((output_data_default[0:half_num_def], output_data_ndef[0:half_num_def]))\n",
    "input_data_test = np.concatenate((input_data_default[half_num_def+1:num_def], input_data_ndef[half_num_def+1:num_def]), axis=0)\n",
    "input_data_test_default = input_data_default[half_num_def+1:num_def]\n",
    "input_data_test_ndef = input_data_ndef[half_num_def+1:num_def]\n",
    "output_data_test = np.concatenate((output_data_default[half_num_def+1:num_def], output_data_ndef[half_num_def+1:num_def]))\n",
    "output_data_test_default = output_data_default[half_num_def+1:num_def]\n",
    "output_data_test_ndef = output_data_ndef[half_num_def+1:num_def]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network implementation\n",
    "For the neural network implementation, we partially followed this [tutorial](http://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html) that offers a good base that we then modified to our needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Declaration of the net\n",
    "It is made of 4 layers, each layers having a number of parameters depending on the number of columns present in the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(col_num,param_num)\n",
    "        self.fc3 = nn.Linear(param_num,param_num)\n",
    "        self.fc4 = nn.Linear(param_num,param_num)\n",
    "        #self.fc5 = nn.Linear(param_num,param_num)\n",
    "        #self.fc6 = nn.Linear(param_num,param_num)\n",
    "        #self.fc7 = nn.Linear(param_num,param_num)\n",
    "        #self.fc8 = nn.Linear(param_num,param_num)\n",
    "        #self.fc9 = nn.Linear(param_num,param_num)\n",
    "        #self.fc10 = nn.Linear(param_num,param_num)\n",
    "        #self.fc11 = nn.Linear(param_num,param_num)\n",
    "        #self.fc12 = nn.Linear(param_num,param_num)\n",
    "        #self.fc13 = nn.Linear(param_num,param_num)\n",
    "        #self.fc14 = nn.Linear(param_num,param_num)\n",
    "        #self.fc15 = nn.Linear(param_num,param_num)\n",
    "        self.fc2 = nn.Linear(param_num,2)\n",
    "        #self.fc4 = nn.Linear(param_num,param_num)\n",
    "        #self.fc5 = nn.Linear(param_num,param_num)\n",
    "        #self.fc6 = nn.Linear(param_num,2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(x)\n",
    "        #m = nn.LeakyReLU()\n",
    "        #x = m(self.fc1(x))\n",
    "        #print(x)\n",
    "        #m = nn.LeakyReLU()\n",
    "        #x = m(self.fc2(x))\n",
    "        #print(x)\n",
    "        #m = nn.LeakyReLU()\n",
    "        #x = m(self.fc3(x))\n",
    "        #print(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        #print(\"LAYER 1\")\n",
    "        #print(x)\n",
    "        #x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        #x = F.relu(self.fc5(x))\n",
    "        #x = F.relu(self.fc6(x))\n",
    "        #x = F.relu(self.fc7(x))\n",
    "        #x = F.relu(self.fc8(x))\n",
    "        #x = F.relu(self.fc9(x))\n",
    "        #x = F.relu(self.fc10(x))\n",
    "        #x = F.relu(self.fc11(x))\n",
    "        #x = F.relu(self.fc12(x))\n",
    "        #x = F.relu(self.fc13(x))\n",
    "        #x = F.relu(self.fc14(x))\n",
    "        #x = F.relu(self.fc15(x))\n",
    "        #print(\"LAYER 2\")\n",
    "        #print(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        #print(\"LAYER 3\")\n",
    "        #print(x)\n",
    "        # Max pooling over a (2, 2) window\n",
    "        #x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        #x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        #x = x.view(-1, self.num_flat_features(x))\n",
    "        #x = F.relu(self.fc1(x))\n",
    "        #x = F.relu(self.fc2(x))\n",
    "        #x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 7 columns in the input data, we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net (\n",
      "  (fc1): Linear (7 -> 7)\n",
      "  (fc3): Linear (7 -> 7)\n",
      "  (fc4): Linear (7 -> 7)\n",
      "  (fc2): Linear (7 -> 2)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the final output is a combination of 2 values.\n",
    "\n",
    "---\n",
    "Part of the net is a set of parameters corresponding to weights associated with each information given as an input. \n",
    "We set the parameters to be between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      " 0.7907  0.0669  0.1690  0.0038  0.8367  0.0138  0.1079\n",
      " 0.4388  0.5240  0.8189  0.2457  0.6980  0.8258  0.6703\n",
      " 0.8013  0.3473  0.1702  0.0716  0.4446  0.6174  0.7090\n",
      " 0.7707  0.5348  0.4092  0.4479  0.4359  0.5640  0.8792\n",
      " 0.7390  0.4351  0.2730  0.8421  0.9516  0.1133  0.8564\n",
      " 0.1610  0.4816  0.5535  0.1207  0.4755  0.3246  0.2840\n",
      " 0.9842  0.2202  0.6269  0.7427  0.7348  0.0188  0.2262\n",
      "[torch.FloatTensor of size 7x7]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "for par in params:\n",
    "    for p in par:\n",
    "        p.data.uniform_(0, 1)\n",
    "print(params[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the neural net on random data\n",
    "The purpose of this is just to make sure that there are no compilation errors when the net is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-0.2852  2.4541 -2.5827 -0.0814  1.9959 -0.0096  0.1887\n",
      " 0.3453  0.3512  0.9091 -0.8473  0.2835 -0.4485 -0.0090\n",
      " 0.1916  0.3022 -0.8995 -1.1556  0.3704  0.0765  1.7199\n",
      "-2.0099  0.1947  1.2209  1.2088  1.7082  0.2674 -1.6513\n",
      " 0.0421 -0.2005  0.1479 -1.2144  0.6888  0.1857 -0.3128\n",
      "[torch.FloatTensor of size 5x7]\n",
      "\n",
      "Variable containing:\n",
      " 0.0647  0.0000\n",
      " 0.1056  0.0000\n",
      " 0.1040  0.0000\n",
      " 0.0908  0.0139\n",
      " 0.1130  0.0000\n",
      "[torch.FloatTensor of size 5x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_net = Net()\n",
    "random_input = Variable(torch.randn(5, col_num))\n",
    "print(random_input)\n",
    "test_out = test_net(random_input)\n",
    "print(test_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create optimizer, set input and target\n",
    "#### Creation of the optimizer with the net parameters and the learning rate\n",
    "The learning rate (lr) is extremely important and sensitive.  \n",
    "It determines how fast the network learns, but setting it too high won't give any interesting data because it will \"bounce around\" without getting more precise data. \n",
    "\n",
    "\n",
    "**It is important to note that we change the learning rate as the learning goes,** indeed, the learning rate needs to be slower (and therefore smaller) as the we get a smaller loss. The learning that you can see is the one used on the latest iteration of our training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# create your optimizer\n",
    "#working version: lr=0.0000001\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.00000001) #the mother of all rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting the input tensor, the output and the target tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input = Variable(torch.from_numpy(input_data_train))\n",
    "input = input.float()\n",
    "output_data_train = np.reshape(output_data_train, -1)\n",
    "target = Variable(torch.from_numpy(output_data_train))\n",
    "#target = target.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "params = list(net.parameters())\n",
    "#print(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Loop\n",
    "- Run the input into the net\n",
    "- Calculate the loss between the output and the target\n",
    "- Calculate the gradient\n",
    "- Optimize the net by modifying the parameters using the gradient and the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS iteration: 0\n",
      "Variable containing:\n",
      " 0.6578\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "LOSS iteration: 100\n",
      "Variable containing:\n",
      " 0.6578\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "LOSS iteration: 200\n",
      "Variable containing:\n",
      " 0.6578\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "LOSS iteration: 300\n",
      "Variable containing:\n",
      " 0.6578\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "LOSS iteration: 400\n",
      "Variable containing:\n",
      " 0.6578\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "LOSS iteration: 500\n",
      "Variable containing:\n",
      " 0.6578\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "LOSS iteration: 600\n",
      "Variable containing:\n",
      " 0.6578\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "LOSS iteration: 700\n",
      "Variable containing:\n",
      " 0.6578\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "LOSS iteration: 800\n",
      "Variable containing:\n",
      " 0.6578\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "LOSS iteration: 900\n",
      "Variable containing:\n",
      " 0.6578\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    # in your training loop:\n",
    "    optimizer.zero_grad()   # zero the gradient buffers\n",
    "\n",
    "    \n",
    "    output = net(input)\n",
    "    #print(output)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    loss = criterion(output, target)\n",
    "    loss.backward()     #calculates gradient\n",
    "    optimizer.step()    # Does the update (applies gradient (multiplies by learning rate and add))\n",
    "    if i % 100 == 0:\n",
    "        print('LOSS iteration: %d' %i)\n",
    "        print(loss)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 0.6578\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4112,)\n"
     ]
    }
   ],
   "source": [
    "print(output_data_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing with training data\n",
    "\n",
    "Testing with training data allows us to verify that our training is functioning, which is also an information available by the loss variable.\n",
    "\n",
    "#### Defaulting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4114, 7])\n",
      "softmax prob\n",
      "Variable containing:\n",
      " 0.4857  0.5143\n",
      " 0.6343  0.3657\n",
      " 0.3701  0.6299\n",
      " 0.5531  0.4469\n",
      " 0.3991  0.6009\n",
      " 0.4170  0.5830\n",
      " 0.4416  0.5584\n",
      " 0.3759  0.6241\n",
      " 0.5164  0.4836\n",
      " 0.5386  0.4614\n",
      "[torch.FloatTensor of size 10x2]\n",
      "\n",
      "Variable containing:\n",
      " 1844.3379\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "4114\n",
      "Variable containing:\n",
      " 0.4483\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "in_data_default = Variable(torch.from_numpy(input_data_train_default))\n",
    "in_data_default = in_data_default.float()\n",
    "\n",
    "print(in_data_default.size())\n",
    "out = net(in_data_default)\n",
    "\n",
    "print('softmax prob')\n",
    "softout = F.softmax(out)\n",
    "print(softout[:10])\n",
    "\n",
    "training_psum_d = 0\n",
    "count_d = 0\n",
    "for o_d in softout:\n",
    "    training_psum_d = training_psum_d + o_d[0]\n",
    "    count_d = count_d + 1\n",
    "    \n",
    "print(training_psum_d)\n",
    "print(count_d)\n",
    "training_psum_d = training_psum_d / count_d\n",
    "print(training_psum_d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the first value of the output of the net, when we give it defaulting inputs, is on average 0.4483.  We will use this result under in order to predict the delinquency of the loan.\n",
    "\n",
    "---\n",
    "#### Not defaulting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax prob\n",
      "Variable containing:\n",
      " 0.5947  0.4053\n",
      " 0.5911  0.4089\n",
      " 0.6246  0.3754\n",
      " 0.5318  0.4682\n",
      " 0.4848  0.5152\n",
      " 0.5718  0.4282\n",
      " 0.5470  0.4530\n",
      " 0.5504  0.4496\n",
      " 0.5643  0.4357\n",
      " 0.6161  0.3839\n",
      "[torch.FloatTensor of size 10x2]\n",
      "\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "Variable containing:\n",
      " 2039.7925\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "4114\n",
      "Variable containing:\n",
      " 0.4958\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "in_data_ndef = Variable(torch.from_numpy(input_data_train_ndef))\n",
    "in_data_ndef = in_data_ndef.float()\n",
    "\n",
    "out = net(in_data_ndef)\n",
    "\n",
    "print('softmax prob')\n",
    "softout = F.softmax(out)\n",
    "print(softout[:10])\n",
    "print(output_data_ndef[:10])\n",
    "\n",
    "training_psum_nd = 0\n",
    "count_nd = 0\n",
    "for o_nd in softout:\n",
    "    training_psum_nd = training_psum_nd + o_nd[0]\n",
    "    count_nd = count_nd + 1\n",
    "    \n",
    "print(training_psum_nd)\n",
    "print(count_nd)\n",
    "training_psum_nd = training_psum_nd / count_nd\n",
    "print(training_psum_nd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the first value of the output of the net, when we give it non-defaulting inputs, is on average 0.4958. We will use this result under in order to predict the delinquency of the loan.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average probability of defaulting is:  Variable containing:\n",
      " 0.4483\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average probability of NOT defaulting is:  Variable containing:\n",
      " 0.4958\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Average probability of defaulting is: ', training_psum_d)\n",
    "print('Average probability of NOT defaulting is: ', training_psum_nd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing with test data\n",
    "We then follow the same steps with our test data: this is data that the net has not been trained on, therefore more accurately testing the net's ability.\n",
    "#### Defaulting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2056, 7])\n",
      "softmax prob\n",
      "Variable containing:\n",
      " 0.4907  0.5093\n",
      " 0.1177  0.8823\n",
      " 0.3367  0.6633\n",
      " 0.3055  0.6945\n",
      " 0.4902  0.5098\n",
      " 0.3239  0.6761\n",
      " 0.5218  0.4782\n",
      " 0.5034  0.4966\n",
      " 0.5438  0.4562\n",
      " 0.4642  0.5358\n",
      "[torch.FloatTensor of size 10x2]\n",
      "\n",
      "Variable containing:\n",
      " 921.5962\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "2056\n",
      "Variable containing:\n",
      " 0.4482\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "in_test_data_default = Variable(torch.from_numpy(input_data_test_default))\n",
    "in_test_data_default = in_test_data_default.float()\n",
    "\n",
    "print(in_test_data_default.size())\n",
    "out = net(in_test_data_default)\n",
    "\n",
    "print('softmax prob')\n",
    "softout = F.softmax(out)\n",
    "print(softout[:10])\n",
    "\n",
    "testing_psum_d = 0\n",
    "count_d = 0\n",
    "for o_d in softout:\n",
    "    testing_psum_d = testing_psum_d + o_d[0]\n",
    "    count_d = count_d + 1\n",
    "    \n",
    "print(testing_psum_d)\n",
    "print(count_d)\n",
    "testing_psum_d = testing_psum_d / count_d\n",
    "print(testing_psum_d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the first value of the output of the net, when we give it defaulting inputs, is on average 0.4482. This is very close to the value obtained from the training data. We will use this result under in order to predict the delinquency of the loan.\n",
    "\n",
    "---\n",
    "#### Not defaulting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax prob\n",
      "Variable containing:\n",
      " 0.5993  0.4007\n",
      " 0.5150  0.4850\n",
      " 0.4359  0.5641\n",
      " 0.5211  0.4789\n",
      " 0.5571  0.4429\n",
      " 0.4637  0.5363\n",
      " 0.3825  0.6175\n",
      " 0.4175  0.5825\n",
      " 0.5992  0.4008\n",
      " 0.4079  0.5921\n",
      "[torch.FloatTensor of size 10x2]\n",
      "\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "Variable containing:\n",
      " 1007.0131\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "2056\n",
      "Variable containing:\n",
      " 0.4898\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "in_test_data_ndef = Variable(torch.from_numpy(input_data_test_ndef))\n",
    "in_test_data_ndef = in_test_data_ndef.float()\n",
    "\n",
    "out = net(in_test_data_ndef)\n",
    "\n",
    "print('softmax prob')\n",
    "softout = F.softmax(out)\n",
    "print(softout[:10])\n",
    "print(output_data_test_ndef[:10])\n",
    "\n",
    "testing_psum_nd = 0\n",
    "count_nd = 0\n",
    "for o_nd in softout:\n",
    "    testing_psum_nd = testing_psum_nd + o_nd[0]\n",
    "    count_nd = count_nd + 1\n",
    "    \n",
    "print(testing_psum_nd)\n",
    "print(count_nd)\n",
    "testing_psum_nd = testing_psum_nd / count_nd\n",
    "print(testing_psum_nd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the first value of the output of the net, when we give it non-defaulting inputs, is on average 0.4888. We will use this result under in order to predict the delinquency of the loan.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average probability of defaulting is:  Variable containing:\n",
      " 0.4482\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Average probability of NOT defaulting is:  Variable containing:\n",
      " 0.4898\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Average probability of defaulting is: ', testing_psum_d)\n",
    "print('Average probability of NOT defaulting is: ', testing_psum_nd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Predicting the delinquency\n",
    "\n",
    "To predict the delinquency, we chose to take the mean between the average of the default values and the average of the non-default values (called cutoff).  \n",
    "Any value that is less than the cutoff will correspond to a prediction that the loan will default, and oppositely, any value above the cutoff corresponds to a prediction that the loan will NOT default.\n",
    "\n",
    "#### Training data\n",
    "We start by trying to predict the delinquency using the training data.  \n",
    "It is likely that we should have been able to get much better results on this situation, given that the neural net can be overfitted to this data.  \n",
    "We were not able to overfit the neural net to the training data, and therefore those are the best results that we have for it. \n",
    "\n",
    "\n",
    "Note that this might be a blessing in disguise as overfitting the neural net usually means poor testing results, and we are satisfied with the result that we obtained using the test data (see next paragraph)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cutoff value is 0.47206\n",
      "softmax prob\n",
      "Variable containing:\n",
      " 0.4857  0.5143\n",
      " 0.6343  0.3657\n",
      " 0.3701  0.6299\n",
      " 0.5531  0.4469\n",
      " 0.3991  0.6009\n",
      " 0.4170  0.5830\n",
      " 0.4416  0.5584\n",
      " 0.3759  0.6241\n",
      " 0.5164  0.4836\n",
      " 0.5386  0.4614\n",
      "[torch.FloatTensor of size 10x2]\n",
      "\n",
      "[1 1 1 1 1 1 1 1 1 1]\n",
      "2588\n",
      "1526\n",
      "Percent of correct:\n",
      "0.629\n",
      "Percent of incorrect:\n",
      "0.371\n"
     ]
    }
   ],
   "source": [
    "cutoff = (training_psum_d + training_psum_nd)/2\n",
    "print(\"cutoff value is %.5f\" % cutoff.data[0])\n",
    "\n",
    "in_train_data = Variable(torch.from_numpy(input_data_train))\n",
    "in_train_data = in_train_data.float()\n",
    "\n",
    "out = net(in_train_data)\n",
    "\n",
    "print('softmax prob')\n",
    "softout = F.softmax(out)\n",
    "print(softout[:10])\n",
    "print(output_data_train[:10])\n",
    "\n",
    "correct_count_train = 0\n",
    "incorrect_count_train = 0\n",
    "count_nd_train = 0\n",
    "for i in range(softout.size()[0]):\n",
    "    o_pred = softout[i][0]\n",
    "    pred_default = o_pred <= cutoff\n",
    "    if (pred_default.data[0] == output_data_train[i]):\n",
    "        correct_count_train += 1\n",
    "    else:\n",
    "        incorrect_count_train += 1\n",
    "    count_nd_train = count_nd_train + 1\n",
    "\n",
    "print(correct_count_train)\n",
    "print(incorrect_count_train)\n",
    "print(\"Percent of correct:\")\n",
    "print('%.3f' %(correct_count_train/count_nd_train))\n",
    "print(\"Percent of incorrect:\")\n",
    "print('%.3f' %(incorrect_count_train/count_nd_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Testing data\n",
    "Using the test data, we find that we are correctly predicting whether a loan will default 59.2% of the time, with the random prediction being 50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cutoff value is 0.47206\n",
      "softmax prob\n",
      "Variable containing:\n",
      " 0.4907  0.5093\n",
      " 0.1177  0.8823\n",
      " 0.3367  0.6633\n",
      " 0.3055  0.6945\n",
      " 0.4902  0.5098\n",
      " 0.3239  0.6761\n",
      " 0.5218  0.4782\n",
      " 0.5034  0.4966\n",
      " 0.5438  0.4562\n",
      " 0.4642  0.5358\n",
      "[torch.FloatTensor of size 10x2]\n",
      "\n",
      "[1 1 1 1 1 1 1 1 1 1]\n",
      "2433\n",
      "1679\n",
      "Percent of correct:\n",
      "0.592\n",
      "Percent of incorrect:\n",
      "0.408\n"
     ]
    }
   ],
   "source": [
    "cutoff = (training_psum_d + training_psum_nd)/2\n",
    "print(\"cutoff value is %.5f\" % cutoff.data[0])\n",
    "\n",
    "in_test_data = Variable(torch.from_numpy(input_data_test))\n",
    "in_test_data = in_test_data.float()\n",
    "\n",
    "out = net(in_test_data)\n",
    "\n",
    "print('softmax prob')\n",
    "softout = F.softmax(out)\n",
    "print(softout[:10])\n",
    "print(output_data_test[:10])\n",
    "\n",
    "correct_count = 0\n",
    "incorrect_count = 0\n",
    "count_nd = 0\n",
    "for i in range(softout.size()[0]):\n",
    "    o_pred = softout[i][0]\n",
    "    pred_default = o_pred <= cutoff\n",
    "    if (pred_default.data[0] == output_data_test[i]):\n",
    "        correct_count += 1\n",
    "    else:\n",
    "        incorrect_count += 1\n",
    "    count_nd = count_nd + 1\n",
    "\n",
    "#print(psum_nd)\n",
    "#print(count_nd)\n",
    "#psum_nd = psum_nd / count_nd\n",
    "print(correct_count)\n",
    "print(incorrect_count)\n",
    "print(\"Percent of correct:\")\n",
    "print('%.3f' %(correct_count/count_nd))\n",
    "print(\"Percent of incorrect:\")\n",
    "print('%.3f' %(incorrect_count/count_nd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Discussion and analysis\n",
    "### Real life application\n",
    "We know that banks and debtors already do a preliminary check to know whether they should give a loan or not. In this dataset, we only had the loan that they accepted, but there are also datasets with the rejects. We decided not to use those sets because the data did not correspond, and we needed to use one of the given sets.  \n",
    "In reality, it is worth it for them to accept a portion of the people that might default as long as this probability is small enough to be worth the profits from the ones not defaulting.  \n",
    "\n",
    "Depending on the accuracy on the current system of prediction of the loans (which is, to our knowledge, mostly based on fixed criterias), it might be possible to improve it significantly. Of course, that would require a better set of data (debtors have this kind of data available) as well as some time sensitive data (not all data is equal between economic stability and crashes).  \n",
    "\n",
    "Another potential use would be to give an idea to creditors as to how much of a loan they can hope to have given their current financial situation. \n",
    "\n",
    "### Suggestions\n",
    "We lack a good understanding of the financial system, specifically the way loans are accepted or refused and the theory behind the value of loaning to some that might default as long as the chance is small enough. Working with someone with knowledge in the banking industry would be helpful.  \n",
    "\n",
    "It would be interesting to give a percentage of chance as to whether the loan will default or not. This should be possible using the standard deviation of the results from the training set to create a scale in which each resul can fall.  \n",
    "\n",
    "We do not have a good explanation as to why we could not get the neural network to overfit the training data. With that knowledge, with would have been certain that everything was working correctly and could have tried to derive the best fitting for testing results from there. Instead, we spent a lot of time trying to get the loss down without too much success.\n",
    "\n",
    "\n",
    "### Authors\n",
    "Brennan Nichyporuk  \n",
    "Vivien Traineau\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
